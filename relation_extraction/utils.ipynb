{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "imperial-trainer",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text = [\"Bill\", \"Gates\", \"founded\", \"Microsoft\", \".\"]\n",
    "h_pos_li = [0, 2]\n",
    "t_pos_li = [3, 4]\n",
    "h_type = None\n",
    "t_type = None\n",
    "h_blank = True\n",
    "t_blank = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "appropriate-justice",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Bill', 'Gates', 'founded', 'Microsoft', '.']"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "interior-intro",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* h * founded ^ t ^ .\n",
      "bill gates\n",
      "microsoft\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb29c925ddcf4f0a89260200bc8edf18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "586107188b4c43c1a8d7e00610a421e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c22162dcb7b400abf73b799284af37c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokens = []\n",
    "h_mention = []\n",
    "t_mention = []\n",
    "\n",
    "for i, token in enumerate(raw_text):\n",
    "    token = token.lower()\n",
    "    if i >= h_pos_li[0] and i < h_pos_li[-1]:\n",
    "        if i == h_pos_li[0]:\n",
    "            tokens += ['*', 'h', '*']\n",
    "        h_mention.append(token)\n",
    "        continue\n",
    "    if i >= t_pos_li[0] and i < t_pos_li[-1]:\n",
    "        if i == t_pos_li[0]:\n",
    "            tokens += ['^', 't' ,'^']\n",
    "        t_mention.append(token)\n",
    "        continue\n",
    "    tokens.append(token)\n",
    "text = \" \".join(tokens)\n",
    "h_mention = ' '.join(h_mention)\n",
    "t_mention = ' '.join(t_mention)\n",
    "\n",
    "print(text)\n",
    "print(h_mention)\n",
    "print(t_mention)\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# tokenize\n",
    "tokenized_text = tokenizer.tokenize(text)\n",
    "tokenized_head = tokenizer.tokenize(h_mention)\n",
    "tokenized_tail = tokenizer.tokenize(t_mention)\n",
    "\n",
    "p_text = \" \".join(tokenized_text)\n",
    "p_head = \" \".join(tokenized_head)\n",
    "p_tail = \" \".join(tokenized_tail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "reported-interest",
   "metadata": {},
   "outputs": [],
   "source": [
    "if h_type != None and t_type != None:\n",
    "    p_head = h_type\n",
    "    p_tail = t_type\n",
    "\n",
    "h_pattern = re.compile(\"\\* h \\*\")\n",
    "t_pattern = re.compile(\"\\^ t \\^\")\n",
    "\n",
    "if h_blank:\n",
    "    p_text = h_pattern.sub(\"[unused0] [unused4] [unused1]\", p_text)\n",
    "else:\n",
    "    p_text = h_pattern.sub(\"[unused0] \" + p_head + \" [unused1]\", p_text)\n",
    "    \n",
    "if t_blank:\n",
    "    p_text = t_pattern.sub(\"[unused2] [unused5] [unused3]\", p_text)\n",
    "else:\n",
    "    p_text = t_pattern.sub(\"[unused2] \" + p_tail + \" [unused3]\", p_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "enormous-maine",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[unused0] [unused4] [unused1] founded [unused2] microsoft [unused3] .'"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "expanded-passport",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bill gates'"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "architectural-sight",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'microsoft'"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_tail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "rational-amino",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', '[unused0]', '[unused4]', '[unused1]', 'founded', '[unused2]', 'microsoft', '[unused3]', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "f_text = (\"[CLS] \" + p_text + \" [SEP]\").split()\n",
    "print(f_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "ordinary-shipping",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 1, 5, 2, 2631, 3, 7513, 4, 1012, 102]"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_input = tokenizer.convert_tokens_to_ids(f_text)\n",
    "tokenized_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "hired-timer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] [unused0] [unused4] [unused1] founded [unused2] microsoft [unused3]. [SEP]'"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenized_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "functional-profit",
   "metadata": {},
   "source": [
    "# 정리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "changed-bumper",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pdb\n",
    "import ast\n",
    "import json\n",
    "import random\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import trange\n",
    "from transformers import BertTokenizer\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "class EntityMarker:\n",
    "    \"\"\"raw text를 BERT-input ids로 바꾸고 entity position을 찾는 클래스.\n",
    "    \n",
    "    Attributes:\n",
    "        tokenizer: Bert-base tokenizer\n",
    "        h_pattern: 정규표현식 패턴 -- * h * 이용. head entity mention을 대체하는데 이용.\n",
    "        t_pattern: 정규표현식 패턴 -- ^ t ^ 이용. tail entity mention을 대체하는데 이용.\n",
    "        err: 정상적으로 head/tail entity를 찾을 수 없는 문장의 개수를 기록\n",
    "        args: command line으로부터의 args    \n",
    "    \"\"\"\n",
    "    def __init__(self, args=None):\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "        self.h_pattern = re.compile(\"\\* h \\*\")\n",
    "        self.t_pattern = re.compile(\"\\^ t \\^\")\n",
    "        self.err = 0\n",
    "        self.args = args\n",
    "        \n",
    "    def tokenize(self, raw_text, h_pos_li, t_pos_li, h_type=None, t_type=None, h_blank=False, t_blank=False):\n",
    "        \"\"\"C+M, C+T, OnlyC setting의 tokenize 함수.\n",
    "        \n",
    "        raw text를 BERT-input ids로 바꾸고, entity-marker를 이용하여 entity 위치를 표시한 뒤,\n",
    "        random하게 entity mention을 [BLANK] symbol로 바꿔준다.\n",
    "        Entity mention은 entity type이 될 수도 있다.\n",
    "        반환하는 값은 BERT에 들어갈 input-ids, entity position.\n",
    "        \n",
    "        Args:\n",
    "            raw_text: tokens가 담긴 리스트.\n",
    "            h_pos_li: head entity position을 담은 리스트. ex) head entity mention이 raw_text[2:6]이면 h_pos_li = [2, 6]\n",
    "            t_pos_li: tail entity position을 담은 리스트.\n",
    "            h_type: head entity type. C+T 세팅 시 이용.\n",
    "            t_type: tail entity type. C+T 세팅 시 이용.\n",
    "            h_blank: head entity mention을 [BLANK]로 바꿀지 말지 여부.\n",
    "            t_blank: tail entity mention을 [BLANK]로 바꿀지 말지 여부.\n",
    "        \n",
    "        Returns:\n",
    "            tokenized_input: BERT에 바로 들어갈 수 있는 input-ids 형태.\n",
    "            h_pos: head entity marker start position\n",
    "            t_pos: tail entity marker start position\n",
    "        \n",
    "        예시:\n",
    "            raw_text: [\"Bill\", \"Gates\", \"founded\", \"Microsoft\", \".\"]\n",
    "            h_pos_li: [0, 2]\n",
    "            t_pos_li: [3, 4]\n",
    "            h_type: None\n",
    "            t_type: None\n",
    "            h_blank: True\n",
    "            t_blank: False\n",
    "            \n",
    "            1. entity mention을 special pattern으로 대체해준다:\n",
    "            \"* h * founded ^ t ^ .\"\n",
    "            \n",
    "            2. pattern을 대체해준다:\n",
    "            \"[CLS] [unused0] [unused4] [unused1] founded [unused2] microsoft [unused3] . [SEP]\"\n",
    "            \n",
    "            3. input id로 변환 및 entity marker start position 찾는다:\n",
    "            [101, 1, 5, 2, 2631, 3, 7513, 4, 1012, 102]\n",
    "            h_pos: 1, t_pos: 5\n",
    "        \"\"\"\n",
    "        tokens = []\n",
    "        h_mention = []\n",
    "        t_mention = []\n",
    "        \n",
    "        for i, token in enumerate(raw_text):\n",
    "            token = token.lower()\n",
    "            if i >= h_pos_li[0] and i < h_pos_li[-1]:\n",
    "                if i == h_pos_li[0]:\n",
    "                    tokens += ['*', 'h', '*']\n",
    "                h_mention.append(token)\n",
    "                continue\n",
    "            if i >= t_pos_li[0] and i < t_pos_li[-1]:\n",
    "                if i == t_pos_li[0]:\n",
    "                    tokens += ['^', 't', '^']\n",
    "                t_mention.append(token)\n",
    "                continue\n",
    "            tokens.append(token)\n",
    "        text = \" \".join(tokens)\n",
    "        h_mention = \" \".join(h_mention)\n",
    "        t_mention = \" \".join(t_mention)\n",
    "        \n",
    "        # tokenize\n",
    "        tokenized_text = self.tokenizer.tokenize(text)\n",
    "        tokenized_head = self.tokenizer.tokenize(h_mention)\n",
    "        tokenized_tail = self.tokenizer.tokenize(t_mention)\n",
    "        \n",
    "        p_text = \" \".join(tokenized_text)\n",
    "        p_head = \" \".join(tokenized_head)\n",
    "        p_tail = \" \".join(tokenized_tail)\n",
    "        \n",
    "        # head entity type과 tail entity type이 None이 아니라면,\n",
    "        # C+T setting을 이용 -> entity mention을 entity type으로 대체\n",
    "        if h_type != None and t_type != None:\n",
    "            p_head = h_type\n",
    "            p_tail = t_type\n",
    "            \n",
    "        # h_blank와 t_blank가 각각 True이면 entity mention을 blank로 대체\n",
    "        if h_blank:\n",
    "            p_text = self.h_pattern.sub(\"[unused0] [unused4] [unused1]\", p_text)\n",
    "        else:\n",
    "            p_text = self.h_pattern.sub(\"[unused0] \" + p_head + \" [unused1]\", p_text)\n",
    "        if t_blank:\n",
    "            p_text = self.t_pattern.sub(\"[unused2] [unused5] [unused3]\", p_text)\n",
    "        else:\n",
    "            p_text = self.t_pattern.sub(\"[unused2] \" + p_tail + \" [unused3]\", p_text)\n",
    "            \n",
    "        f_text = (\"[CLS] \" + p_text + \" [SEP]\").split()\n",
    "        \n",
    "        # 만약 h_pos_li와 t_pos_li에서 overlap이 발생하면, head entity와 tail entity를 제대로 찾을 수 없음\n",
    "        try:\n",
    "            h_pos = f_text.index(\"[unused0]\")\n",
    "        except:\n",
    "            self.err += 1\n",
    "            h_pos = 0\n",
    "        try:\n",
    "            t_pos = f_text.index(\"[unused2]\")\n",
    "        except:\n",
    "            self.err += 1\n",
    "            t_pos = 0\n",
    "            \n",
    "        tokenized_input = self.tokenizer.convert_tokens_to_ids(f_text)\n",
    "        \n",
    "        return tokenized_input, h_pos, t_pos\n",
    "    \n",
    "    def tokenize_OMOT(self, tokenized_head, tokenized_tail, h_first):\n",
    "        '''OnlyM, OnlyT setting의 tokenize 함수.\n",
    "        \n",
    "        head entity와 tail entity를 id로 바꿔준다.\n",
    "        \n",
    "        Args:\n",
    "            tokenized_head: Head entity mention 또는 type을 리스트 형태로 담고 있음. BertTokenizer로 tokenized.\n",
    "            tokenized_tail: Tail entity mention 또는 type을 리스트 형태로 담고 있음. BertTOkenizer로 tokenized.\n",
    "            h_first: head entity가 첫 번째 entity인지의 여부\n",
    "            \n",
    "        Returns:\n",
    "            tokenized_input: BERT에 바로 들어갈 수 있는 input-ids 형태.\n",
    "            h_pos: head entity marker start position\n",
    "            t_pos: tail entity marker start position\n",
    "        '''\n",
    "        \n",
    "        tokens = [\"[CLS]\",]\n",
    "        \n",
    "        if h_first:\n",
    "            h_pos = 1\n",
    "            tokens += [\"[unused0]\",] + tokenized_head + [\"[unused1]\",]\n",
    "            t_pos = len(tokens)\n",
    "            tokens += [\"[unused2]\",] + tokenized_tail + [\"[unused3]\",]\n",
    "        else:\n",
    "            t_pos = 1\n",
    "            tokens += [\"[unused2]\",] + tokenized_tail + [\"[unused3]\",]\n",
    "            h_pos = len(tokens)\n",
    "            tokens += [\"[unused0]\",] + tokenized_head + [\"[unused1]\",]\n",
    "            \n",
    "        tokens.append(\"[SEP]\")\n",
    "        \n",
    "        tokenized_input = tokenizer.convert_tokens_to_ids(tokens)\n",
    "            \n",
    "        return tokenized_input, h_pos, t_pos    \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "impressed-british",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modified-belize",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mysterious-philosophy",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
